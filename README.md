# ğŸ“Š Azure Data Factory â€“ ETL de fichier CSV vers Azure SQL Database

Ce projet montre comment configurer un pipeline ETL simple avec **Azure Data Factory (ADF)** pour transfÃ©rer des donnÃ©es depuis un fichier CSV stockÃ© dans **Azure Blob Storage** vers une table dans **Azure SQL Database**, le tout via l'interface graphique d'Azure sans Ã©crire de code.

---

## ğŸ§­ Objectif

CrÃ©er un pipeline dans ADF qui :
- Extrait des donnÃ©es Ã  partir dâ€™un fichier CSV dans Azure Blob Storage
- Les charge dans une table relationnelle `Cars` dans Azure SQL
- Le tout en utilisant **Linked Services**, **Datasets** et une **Copy Activity**

---

## ğŸ“Œ Vue dâ€™ensemble du pipeline

![Diagramme complet du pipeline](screenshots/Capture d'Ã©cran 2025-04-08 155448.png)

Ce diagramme illustre lâ€™architecture globale du pipeline :
- Source : Blob Storage contenant `Cars.csv`
- ActivitÃ© de copie orchestrÃ©e par ADF
- Destination : table `Cars` dans Azure SQL

---

## ğŸ§± Ã‰tapes du projet

### 1. CrÃ©ation du service Azure Data Factory

- CrÃ©e une ressource ADF depuis le portail Azure
- Remplis les champs : nom, groupe de ressources, rÃ©gion
- Clique sur **Author & Monitor** pour accÃ©der Ã  l'interface graphique

---

### 2. PrÃ©paration des donnÃ©es sources et cibles

- TÃ©lÃ©verse le fichier `cars.csv` dans un conteneur Azure Blob Storage
- CrÃ©e la table cible dans Azure SQL Database avec ce script :
sql
CREATE TABLE Cars (
  Make NVARCHAR(50),
  Model NVARCHAR(50),
  Type NVARCHAR(50),
  Origin NVARCHAR(50),
  DriveTrain NVARCHAR(50),
  Length FLOAT
);


### 3. ğŸ”— CrÃ©ation des Linked Services

Les **Linked Services** sont des connexions sÃ©curisÃ©es vers les sources et destinations de donnÃ©es.

- ğŸ” CrÃ©e un Linked Service vers **Azure Blob Storage**
  - MÃ©thode dâ€™authentification : Account Key ou SAS Token
- ğŸ” CrÃ©e un Linked Service vers **Azure SQL Database**
  - MÃ©thode dâ€™authentification : SQL Login (utilisateur + mot de passe)
  - Assure-toi que le firewall du serveur SQL autorise les services Azure

---

### 4. ğŸ“¦ DÃ©finition des Datasets

Les **datasets** reprÃ©sentent les donnÃ©es manipulÃ©es dans le pipeline.

- Dataset source :
  - Type : Azure Blob Storage â€“ DelimitedText (CSV)
  - Fichier : `cars.csv`
  - Option cochÃ©e : *First row as header*
- Dataset destination :
  - Type : Azure SQL Database
  - Table cible : `Cars`

---

### 5. ğŸ› ï¸ CrÃ©ation du pipeline et configuration de la Copy Activity

- CrÃ©e un **pipeline** (nommÃ© par ex. `CopyCarsPipeline`)
- Ajoute une **Copy Data Activity**
- SÃ©lectionne :
  - Le dataset source (`cars.csv`)
  - Le dataset cible (`Cars`)
- Va dans lâ€™onglet **Mapping** pour vÃ©rifier que les colonnes sont bien mappÃ©es (automatiquement ou manuellement)

ğŸ“· **Exemple avancÃ© : plusieurs activitÃ©s de copie dans un pipeline**

![Exemple avancÃ© avec plusieurs Copy Activities](screenshots/Capture d'Ã©cran 2025-04-08 155519.png)

---

### 6. â–¶ï¸ ExÃ©cution et suivi du pipeline

- Lance un **Debug** dans lâ€™interface dâ€™ADF
- Ouvre lâ€™onglet **Monitor** pour :
  - Suivre lâ€™Ã©tat dâ€™exÃ©cution
  - VÃ©rifier que lâ€™activitÃ© est bien en *Success*
  - Obtenir le nombre de lignes transfÃ©rÃ©es

---

### 7. ğŸ§¾ VÃ©rification des donnÃ©es dans Azure SQL Database

- Connecte-toi Ã  ton Azure SQL Database
- ExÃ©cute la requÃªte suivante pour voir les donnÃ©es importÃ©es :
sql
SELECT * FROM Cars;

![RÃ©sultat d'importation dans SQL](screenshots/Capture d'Ã©cran 2025-04-08 155642.png)



### ğŸ§  RÃ©sultat attendu

ğŸ“· **Diagramme illustrant lâ€™architecture complÃ¨te du pipeline :**

![Pipeline complet : CSV â†’ SQL](screenshots/Capture d'Ã©cran 2025-04-08 155448.png)

Ce projet aboutit Ã  un pipeline fonctionnel dans Azure Data Factory capable de :

- Lire un fichier CSV (`cars.csv`) stockÃ© dans Azure Blob Storage
- Copier automatiquement les donnÃ©es dans une table (`Cars`) dâ€™une base de donnÃ©es Azure SQL
- GÃ©rer lâ€™ensemble du processus sans avoir besoin dâ€™Ã©crire une seule ligne de code

Ce pipeline peut ensuite Ãªtre :
- ExÃ©cutÃ© Ã  la demande
- ProgrammÃ© pour s'exÃ©cuter automatiquement
- Ã‰tendu pour d'autres types de donnÃ©es ou destinations

---

### ğŸš€ AmÃ©liorations possibles

Voici quelques idÃ©es pour enrichir ce projet de base :

- ğŸ”„ **Automatisation** : ajouter des triggers pour exÃ©cuter le pipeline Ã  des intervalles rÃ©guliers (quotidien, horaire...) ou Ã  l'arrivÃ©e d'un nouveau fichier
- ğŸ§ª **Ajout de transformations** : intÃ©grer des **Mapping Data Flows** pour transformer les donnÃ©es pendant leur transit
- ğŸ“ **Multi-fichiers** : configurer le pipeline pour traiter plusieurs fichiers CSV Ã  la fois via des wildcards ou un paramÃ©trage dynamique
- ğŸ§© **ModularitÃ©** : paramÃ©trer le pipeline pour le rendre rÃ©utilisable avec diffÃ©rents noms de fichiers, schÃ©mas ou destinations

---

### ğŸ™ CrÃ©dit

Ce tutoriel est **inspirÃ©** du travail de la chaÃ®ne YouTube :

ğŸ¥ [Adam Marczak â€“ Azure for Everyone](https://www.youtube.com/@AdamMarczak)  
ğŸ“º VidÃ©o originale : [Azure Data Factory Tutorial â€“ ETL Made Easy](https://www.youtube.com/watch?v=EpDkxTHAhOs)

L'objectif ici est pÃ©dagogique : reproduire et adapter le projet Ã  des fins dâ€™apprentissage.

---
